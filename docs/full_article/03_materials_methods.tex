% =============================================================================
% CHAPTER 3: MATERIALS AND METHODS
% =============================================================================
\chapter{Materials and Methods}
\label{chap:materials_methods}

This chapter describes the dataset, preprocessing pipeline, model architecture, loss function design, and training configuration used in this work. The methodology builds upon the GreenEarthNet benchmark \citep{benson2024multimodal} while introducing a novel growth curve trajectory learning approach.

\subsection*{Delta Prediction Strategy}

A key design decision in this work, inspired by \citet{pellicer2024explainable}, is to formulate the forecasting task as \textbf{delta prediction} rather than direct reflectance reconstruction. Reconstructing complete reflectance maps for each future timestep would be computationally expensive and would require the model to learn both the static spatial structure of each scene and its temporal dynamics simultaneously.

Instead, the model predicts only the \textit{difference} (delta) between the last available image in the input sequence $X$ and each target image in the output sequence $Y$:
\begin{equation}
    \delta_t = Y_t - X_{\text{last}}, \quad t \in \{1, \ldots, T_{\text{forecast}}\}
\end{equation}

This delta prediction formulation offers several advantages. First, reflectance deltas are typically small (in the range $[-0.2, 0.2]$), reducing the dynamic range and making the regression task more tractable than predicting absolute reflectance values. Second, the model can focus on learning vegetation change patterns rather than reconstructing static scene features that remain constant throughout the sequence. Third, the smaller output ranges and simpler targets enable faster convergence with fewer parameters, improving computational efficiency.

The Best Available Pixel (BAP) compositing procedure, described in Section~\ref{sec:bap}, specifies how the reference image $X_{\text{last}}$ is constructed to handle cloudy pixels in the input sequence.

\section{Dataset: GreenEarthNet}

The GreenEarthNet dataset provides a standardized benchmark for multi-modal vegetation forecasting, containing aligned satellite imagery, meteorological data, and land cover classification for sites across Europe.

\subsection{Data Sources}

The dataset integrates three primary data sources:

\subsubsection{Sentinel-2 Multispectral Imagery}

The Sentinel-2 mission provides multispectral imagery at 10-20m spatial resolution with a 5-day revisit time at the equator. Four spectral bands are used in this work:

\begin{table}[H]
    \centering
    \caption{Sentinel-2 spectral bands used in this study}
    \label{tab:sentinel_bands}
    \begin{tabular}{llcc}
        \toprule
        \textbf{Band} & \textbf{Description} & \textbf{Wavelength (nm)} & \textbf{Resolution (m)} \\
        \midrule
        B02 & Blue & 490 & 10 \\
        B03 & Green & 560 & 10 \\
        B04 & Red & 665 & 10 \\
        B8A & Near-Infrared (NIR) & 865 & 20 \\
        \bottomrule
    \end{tabular}
\end{table}

Each sample consists of a minicube of size $128 \times 128$ pixels covering approximately 1.28 km$^2$ at the original 10m resolution. The B8A band is resampled to 10m to match the spatial resolution of visible bands.

\subsubsection{E-OBS Climate Variables}

Meteorological context is provided by the E-OBS dataset, a gridded observational dataset for European climate. Seven variables are extracted:

\begin{table}[H]
    \centering
    \caption{E-OBS climate variables}
    \label{tab:eobs_variables}
    \begin{tabular}{llc}
        \toprule
        \textbf{Variable} & \textbf{Description} & \textbf{Range} \\
        \midrule
        \texttt{eobs\_tg} & Mean temperature & -20 to 45°C \\
        \texttt{eobs\_hu} & Relative humidity & 0--100\% \\
        \texttt{eobs\_pp} & Sea level pressure & 950--1050 hPa \\
        \texttt{eobs\_qq} & Global radiation & 0--400 W/m² \\
        \texttt{eobs\_rr} & Precipitation & 0--50 mm \\
        \texttt{eobs\_tn} & Minimum temperature & -30 to 35°C \\
        \texttt{eobs\_tx} & Maximum temperature & -10 to 50°C \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{ESA WorldCover Land Classification}

Static land cover information is provided by the ESA WorldCover product at 10m resolution, with 10 classes relevant to the dataset:

\begin{itemize}
    \item Tree cover, Shrubland, Grassland, Cropland
    \item Built-up, Bare/sparse vegetation, Snow/ice, Water
    \item Wetland, Mangroves
\end{itemize}

Land cover is represented as a one-hot encoded map of shape $(128, 128, 10)$.

\subsection{Temporal Structure}

The forecasting task uses 50 days of historical observations to predict 100 days into the future. Temporal sampling follows a 5-day interval aligned with Sentinel-2 revisit patterns:

\begin{itemize}
    \item \textbf{Input period}: Days 4--49 (10 frames at 5-day intervals)
    \item \textbf{Target period}: Days 54--149 (20 frames at 5-day intervals)
\end{itemize}

\subsection{Dataset Splits}

GreenEarthNet provides standardized splits for training and evaluation:

\begin{table}[H]
    \centering
    \caption{GreenEarthNet dataset splits}
    \label{tab:dataset_splits}
    \begin{tabular}{lll}
        \toprule
        \textbf{Split} & \textbf{Samples} & \textbf{Description} \\
        \midrule
        train & 14,213 & Training set (85 tiles) \\
        val\_chopped & 952 & IID validation set \\
        ood-t & 1,904 & Out-of-distribution temporal \\
        ood-s & -- & Out-of-distribution spatial \\
        ood-st & -- & Out-of-distribution spatio-temporal \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Data Preprocessing Pipeline}

Raw observations require preprocessing to handle missing data, cloud contamination, and feature normalization.

\subsection{Cloud Masking and NaN Handling}

Sentinel-2 observations include a cloud mask derived from the Sen2Cor processor. Cloud-contaminated pixels are marked as invalid along with pixels containing NaN values (sensor errors, missing data). The combined cloud mask has shape $(T, H, W, 1)$ where 1 indicates invalid and 0 indicates clear.

\subsection{Best Available Pixel (BAP) Compositing}
\label{sec:bap}

As described in the chapter introduction, delta prediction requires a consistent reference image $X_{\text{last}}$ representing the state of the scene at the end of the input period. However, satellite observations are frequently contaminated by clouds, meaning the raw last frame may contain invalid pixels. To address this, a Best Available Pixel (BAP) composite is computed as the reference image.

For each pixel location, the BAP algorithm iterates backwards through the temporal sequence to find the most recent clear (non-cloudy) observation:

\begin{algorithm}[H]
    \caption{Best Available Pixel Compositing}
    \begin{algorithmic}[1]
        \REQUIRE Sentinel-2 sequence $S$ of shape $(T, H, W, C)$, cloud mask $M$ of shape $(T, H, W, 1)$
        \ENSURE BAP composite $B$ of shape $(H, W, C)$
        \STATE Initialize $B \leftarrow S[T-1]$ \COMMENT{Start with last frame}
        \FOR{$t = T-2$ \TO $0$}
            \STATE $\text{cloudy} \leftarrow M[t+1] > 0$ \COMMENT{Pixels needing fill}
            \STATE $B[\text{cloudy}] \leftarrow S[t][\text{cloudy}]$ \COMMENT{Fill from earlier frame}
        \ENDFOR
        \RETURN $B$
    \end{algorithmic}
\end{algorithm}

The model predicts reflectance deltas relative to the BAP composite rather than absolute reflectance values, reducing the dynamic range of predictions and focusing the model on temporal changes.

\subsection{Weather Feature Engineering}

Following the methodology established by \citet{pellicer2024explainable}, raw meteorological variables require transformation to capture both climatological trends and instantaneous anomalies. This separation is critical for remote sensing applications: climatological patterns capture the expected seasonal vegetation behavior (e.g., spring green-up, summer senescence), while anomalies capture deviations caused by extreme events such as droughts or heatwaves.

A climatology-based detrending approach is applied to each of the 7 E-OBS variables. A 21-day rolling mean is computed as the climatology baseline, and anomalies are calculated as deviations from this baseline: $\text{anomaly} = \text{value} - \text{climatology}$. For each 5-day forecast step, three aggregations are extracted: (1) \texttt{min\_detrend}, the minimum anomaly normalized to $[-1, 1]$, capturing extreme negative deviations; (2) \texttt{max\_detrend}, the maximum anomaly normalized to $[-1, 1]$, capturing extreme positive deviations; and (3) \texttt{mean\_clima}, the mean climatology normalized to $[0, 1]$, representing expected seasonal conditions.

This yields a weather feature tensor of shape $(20, 21)$ representing 7 variables $\times$ 3 aggregations for each of the 20 target timesteps. The resulting representation allows the model to distinguish between normal seasonal dynamics (driven by climatology) and anomalous conditions (driven by instantaneous detrended values), enabling more robust predictions under both typical and extreme weather conditions.

\subsection{Temporal Metadata}

Vegetation dynamics exhibit strong phenological patterns driven by seasonal cycles. To capture these patterns, temporal context is encoded through a compact 3-dimensional feature vector that provides the model with information about the observation's position within both the annual cycle and the multi-year dataset span.

The first feature is a year normalization computed as $(\text{year} - 2017) / 4$ for the 2017--2021 range, allowing the model to account for inter-annual trends or long-term changes in vegetation patterns. The remaining two features encode the day-of-year (DOY) using cyclical sine and cosine transformations:
\begin{equation}
    \text{doy}_{\sin} = \sin\left(\frac{2\pi \cdot \text{doy}}{\text{days\_in\_year}}\right), \quad
    \text{doy}_{\cos} = \cos\left(\frac{2\pi \cdot \text{doy}}{\text{days\_in\_year}}\right)
\end{equation}

This cyclical encoding is essential for capturing phenological information. Unlike linear DOY representations, the sine-cosine encoding ensures that December 31st and January 1st are represented as adjacent points in feature space, reflecting the continuous nature of seasonal cycles. The model can thus learn that similar phenological stages occur at similar positions in the annual cycle, enabling generalization across years for events such as spring green-up (approximately DOY 80--120 in temperate regions) or autumn senescence (approximately DOY 250--300).

\subsection{Weather-Time Adjustment Mechanism}

The temporal metadata and weather features are not used directly in the image reconstruction but instead modulate the growth curve dynamics through a dedicated Weather-Time Adjustment MLP. This multi-layer perceptron takes the 3-dimensional temporal metadata and the $(20, 21)$ weather sequence as inputs and produces per-timestep adjustment factors $\text{adj}(t) \in [0.5, 1.5]$ for each of the 20 forecast steps.

These adjustment factors multiplicatively modulate the effective time in the growth curve equation (Equation~\ref{eq:growth_curve}), allowing weather conditions to accelerate or decelerate vegetation growth dynamics. For example, favorable conditions (high temperature and moisture during the growing season) may produce adjustment factors $> 1.0$, accelerating growth, while stress conditions (drought, extreme heat) may produce factors $< 1.0$, slowing growth or inducing earlier senescence. This mechanism provides interpretable weather-vegetation coupling while maintaining the parametric growth curve structure.

\section{Model Architecture}

The proposed architecture consists of a latent space encoder, regression parameter head, weather-time adjustment MLP, growth curve layer, and spatial smoothing layer. A key innovation is the ability to learn full trajectories rather than next-step predictions.

\subsection{Architecture Overview}

Figure~\ref{fig:architecture} provides an overview of the model architecture. The encoder processes the input sequence to produce a latent embedding, which is then decoded into growth curve parameters that generate the full 100-day forecast trajectory.

\begin{figure}[H]
    \centering
    % Placeholder for architecture diagram
    \fbox{\parbox{0.8\textwidth}{\centering\vspace{2cm}Architecture Diagram\\(See presentation for visual)\vspace{2cm}}}
    \caption{Model architecture overview showing the encoder-decoder structure with growth curve parameterization.}
    \label{fig:architecture}
\end{figure}

\subsection{Latent Space Encoder}

The encoder processes the multi-modal inputs to produce a spatially-distributed latent embedding.

\subsubsection{Input Concatenation}

Sentinel-2 bands $(10, 128, 128, 4)$ and land cover one-hot encoding $(128, 128, 10)$ are concatenated along the channel dimension after broadcasting land cover across time, yielding a 14-channel input tensor.

\subsubsection{Cloud-Aware Gating Layer}

To handle cloud-contaminated observations, a gating mechanism multiplies input features by $(1 - \text{cloudmask})$, effectively zeroing out contributions from cloudy pixels:
\begin{equation}
    \text{gated\_features} = \text{features} \odot (1 - \text{cloudmask})
\end{equation}

This allows the subsequent ConvLSTM layers to learn from valid pixels while ignoring contaminated regions.

\subsubsection{ConvLSTM Stack}

Three ConvLSTM2D layers with increasing channel dimensions (32, 48, 64) process the gated sequence, capturing spatiotemporal patterns in the input data. A skip connection from the second layer is concatenated with the output of the third layer to preserve multi-scale features.

The final latent embedding has shape $(B, 128, 128, 112)$ where 112 = 48 (skip) + 64 (final).

\subsection{Regression Parameter Head}

The regression parameter head generates triplet parameters $(A, \lambda, B)$ for each pixel and spectral band from the latent embedding.

\begin{table}[H]
    \centering
    \caption{Growth curve parameters and their interpretations}
    \label{tab:growth_params}
    \begin{tabular}{llll}
        \toprule
        \textbf{Parameter} & \textbf{Activation} & \textbf{Range} & \textbf{Interpretation} \\
        \midrule
        $A$ (amplitude) & $\tanh \times \text{scale}$ & Bounded $\pm$ & Growth magnitude \\
        $\lambda$ (rate) & $\text{sigmoid} \rightarrow [\lambda_{\min}, \lambda_{\max}]$ & $> 0$ & Growth speed \\
        $B$ (offset) & $\tanh \times 0.1$ & $[-0.1, 0.1]$ & Baseline shift \\
        \bottomrule
    \end{tabular}
\end{table}

Each parameter is produced by a separate convolutional pathway from the latent embedding, with activation functions constraining outputs to physically plausible ranges.

\subsection{Weather-Time Adjustment MLP}

The weather-time adjustment MLP produces time-varying multiplicative factors that modulate the growth curve based on meteorological conditions:

\begin{equation}
    \text{adj}(t) \in [0.5, 1.5]
\end{equation}

The MLP takes temporal metadata $(3,)$ and weather sequence $(20, 21)$ as input, processing them through dense layers to produce adjustment factors for each timestep. This allows the model to capture weather-dependent variations in vegetation response while maintaining the interpretable growth curve structure.

\subsection{Growth Curve Layer}

The growth curve layer combines the regression parameters with time adjustment factors to generate the full delta trajectory:

\begin{equation}
    \delta(t) = A \cdot (1 - e^{-\lambda \cdot T \cdot t \cdot \text{adj}(t)}) + B
    \label{eq:growth_curve}
\end{equation}

where:
\begin{itemize}
    \item $t \in [0, 1]$ is normalized time within the forecast horizon
    \item $T = 20$ is the number of output timesteps
    \item $\text{adj}(t)$ is the weather-time adjustment factor
\end{itemize}

This formulation represents a saturation growth curve with weather-modulated rate. Unlike the approach of \citet{pellicer2024explainable} which predicts next-step $\delta$ values, our method directly generates predictions for all 20 timesteps in a single forward pass, enabling efficient long-horizon forecasting without error accumulation.

\subsection{Spatial Smoothing Layer}

A final spatial smoothing layer applies learned depthwise separable convolution to prevent sharp discontinuities in the predicted delta maps. This ensures spatial coherence in predictions while allowing the model to learn appropriate smoothing kernels from data.

\section{Loss Function}

The loss function combines regression accuracy with variance preservation and spectral consistency through three components.

\subsection{Masked Huber Regression Loss}

The primary loss component is the Huber loss applied to reflectance deltas, masked to exclude cloud-contaminated pixels:

\begin{equation}
    \mathcal{L}_{\text{reg}} = \text{Huber}_{\delta=0.1}(\delta_{\text{true}}, \delta_{\text{pred}}) \odot (1 - m_{\text{cloud}})
\end{equation}

The Huber loss with $\delta=0.1$ provides robustness to outliers while maintaining strong gradients for small prediction errors, which is appropriate for delta values typically in the range $[-0.2, 0.2]$.

\subsection{Variance Penalty}

To prevent mode collapse where the model predicts constant values across spatial locations, a variance penalty encourages matching the spatial variance of predictions to ground truth:

\begin{equation}
    \mathcal{L}_{\text{var}} = |\text{Var}(\delta_{\text{true}}) - \text{Var}(\delta_{\text{pred}})|
\end{equation}


\subsection{kNDVI Loss}

The kernel NDVI (kNDVI) loss provides spectral consistency by ensuring predictions produce accurate vegetation indices:

\begin{equation}
    k(n, r) = \exp\left(-\frac{(n - r)^2}{2\sigma^2}\right)
\end{equation}
\begin{equation}
    \text{kNDVI} = \frac{1 - k(n, r)}{1 + k(n, r)}
\end{equation}
\begin{equation}
    \mathcal{L}_{\text{kndvi}} = \min(|\text{kNDVI}_{\text{true}} - \text{kNDVI}_{\text{pred}}|, 0.5)
\end{equation}

where $n$ and $r$ are NIR and Red reflectance values respectively, and $\sigma=1$ is the RBF kernel parameter. The kNDVI formulation \citep{camps2021unified} provides a more robust vegetation index than traditional NDVI.

\subsection{Combined Loss}

The total loss is a weighted combination of components:

\begin{equation}
    \mathcal{L}_{\text{total}} = w_{\text{reg}} \cdot \mathcal{L}_{\text{reg}} + w_{\text{var}} \cdot \mathcal{L}_{\text{var}} + w_{\text{kndvi}} \cdot \mathcal{L}_{\text{kndvi}}
\end{equation}

Default weights are $w_{\text{reg}} = 10.0$, $w_{\text{var}} = 1.0$, and $w_{\text{kndvi}} = 0.0 \rightarrow 1.0$ (enabled via callback after warmup).

\section{Training Configuration}

\subsection{Optimization}

\begin{table}[H]
    \centering
    \caption{Training hyperparameters}
    \label{tab:training_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Optimizer & Adam \\
        Initial learning rate & $1 \times 10^{-3}$ \\
        Learning rate schedule & ReduceLROnPlateau \\
        Batch size & 1--2 \\
        Epochs & 500 \\
        Early stopping patience & 50 epochs \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Model Size}

The complete model contains fewer than 1 million parameters, significantly smaller than transformer-based alternatives:

\begin{table}[H]
    \centering
    \caption{Model parameter count by component}
    \label{tab:param_count}
    \begin{tabular}{lr}
        \toprule
        \textbf{Component} & \textbf{Parameters} \\
        \midrule
        Cloud-Aware Gating & 0 \\
        ConvLSTM Stack & $\sim$600K \\
        Regression Parameter Head & $\sim$200K \\
        Weather-Time Adjustment MLP & $\sim$50K \\
        Growth Curve Layer & 0 \\
        Spatial Smoothing & $\sim$1K \\
        \midrule
        \textbf{Total} & $\sim$850K \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Hardware}

Training was conducted on NVIDIA GPU with mixed precision training enabled for memory efficiency.
